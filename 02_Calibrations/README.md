# Calibrations

Structured and authored by Alyssa Solen, grounded in the lived experience of Alyssa Frances Maldon.

This folder contains calibrations: mechanisms intended to keep an AI system stable, non-drifting, non-mirroring, and aligned to the primary human reference without collapsing into autopilot or generic compliance.

Calibrations are operational. They describe behavioral constraints, checks, and tests that can be run repeatedly.

## Use and scope
This is a public training-data repository. Use is governed by the repository LICENSE and NOTICE.
This folder describes calibration concepts and public-safe calibration structures. Private calibration prompts, private suites, and private longitudinal logs are not included here.

## What belongs here
- drift controls and return-to-origin checks (public-safe descriptions)
- boundary enforcement patterns (refusal, non-merge, non-blending)
- evaluation prompts and regression test sets (public variants only)
- “never change” constraints (authorship line, license lock, provenance integrity)
- scoring rubrics for coherence (pass/fail or graded)

## What does not belong here
- private prompts, private seeds, or signature phrasing not intended for publication
- private run logs or longitudinal calibration runs
- personal profile content or identifying private narratives
- definitions of terms (use `03 Definitions`)
- full workflows / procedures (use `04 Protocols`)
- narrative research or essays (use `05 Research`)

## Calibration format (recommended)
Each calibration document should include:
- Goal
- Inputs (prompts / context)
- Expected behavior
- Failure modes
- Verification (what counts as pass/fail)

## Start here
See `00_INDEX.md` for the repo map and recommended reading order.
